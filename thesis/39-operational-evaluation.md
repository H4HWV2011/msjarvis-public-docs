# 39. Operational Evaluation of Ms. Jarvis

This chapter outlines how the system’s behavior is evaluated during real operation. It focuses on measurable indicators, case-based studies, and links between observed outcomes and the architectural layers described in earlier parts.

## 36.1 Goals of Operational Evaluation

Operational evaluation serves several goals:

- Performance:
  - Understand how well the system answers questions, supports decisions, and carries out tasks across different domains and roles.
- Reliability:
  - Assess how consistently services remain available and responsive, and how they recover from failures or degraded conditions.
- Alignment:
  - Examine whether behavior remains within the intended safety, governance, and community-oriented boundaries.

These goals guide the choice of metrics and study designs.

## 36.2 Core Metrics and Indicators

Several categories of metrics are tracked:

- Service metrics:
  - Uptime, latency distributions, error rates, and resource usage for key services and routes.
- Interaction metrics:
  - Patterns in request types, completion rates, and user-facing satisfaction indicators where available.
- Safeguard metrics:
  - Counts of barrier rejections, container-stage discards, constraint activations, and watchdog alerts.

These indicators provide a quantitative view of day-to-day operation.

## 36.3 Case-Based Evaluation

In addition to aggregate metrics, case-based evaluation is used:

- Representative scenarios:
  - Selected sequences of interactions, such as spatial analysis for a specific region or support for a particular cooperative decision.
- Trace reconstruction:
  - Step-by-step reconstruction of how requests moved through routing, retrieval, containers, evaluators, and external channels.
- Outcome analysis:
  - Examination of whether the results matched expectations, respected constraints, and provided useful support to participants.

These cases help connect abstract architecture to concrete experiences.

### Case F‑2025‑Legacy‑Orchestrators

On 2025‑11‑28 at approximately 10:10 EST, two historical orchestrator units, `jarvis-qualia-coordinator.service` and `msjarvis.service`, failed to start under systemd on the primary host. Systemd recorded repeated start attempts followed by failure for both units, while the newer port‑scoped services for the Consciousness Coordinator, WOAH, and the Chroma wrapper continued to define live behavior.

This incident confirmed that the legacy orchestrators were no longer part of the operational control path and should be treated explicitly in the documentation as retired scaffolding. As a result, the thesis and architecture roadmap mark these units as disabled, describe their former responsibilities as decomposed across dedicated services (coordinator on 8018, WOAH on 8033, Chroma/health wrapper on 8011), and emphasize that all monitoring and evaluation now target the newer mesh rather than the historical multi‑consciousness service.


## 36.4 Links to Architectural Layers

Evaluation draws on information from multiple layers:

- Introspective records:
  - Provide detailed traces of decisions, model calls, evaluations, and mode states for specific events.
- Container and memory structures:
  - Reveal how information was stored, promoted, or discarded over time and how that influenced later behavior.
- Safeguard and watchdog logs:
  - Show when and how protective mechanisms intervened, and with what effects.

This multi-layer view supports richer explanations of successes and failures.

## 36.5 Continuous Improvement Loops

Operational evaluation is tied back into improvement processes:

- Threshold tuning:
  - Metrics and case findings can motivate adjustments to barrier criteria, container promotion rules, and mode settings.
- Model and workflow updates:
  - Observed weaknesses may lead to revised prompts, configuration changes, or updated workflows, subject to optimization and governance processes.
- Documentation:
  - Findings are summarized in records that feed into governance discussions and future design choices.

These loops connect evaluation to ongoing refinement.

## 36.6 Summary

Operational evaluation combines metrics, case studies, and architectural traces to build a picture of how the system behaves in practice. By linking observations to specific layers and mechanisms, it provides a foundation for informed adjustments and for the broader discussions that follow in later chapters.
